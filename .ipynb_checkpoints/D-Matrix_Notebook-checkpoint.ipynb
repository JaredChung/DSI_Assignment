{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSI Assessment 2 - Quantified Self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment will be using the CRISP DM Framework to complete the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Contents Page\n",
    "\n",
    "* Business Understanding\n",
    "* Data Knowledge\n",
    "* Data Processing\n",
    "* Data Exploration\n",
    "* \n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is about exporting the information and processing it in a matter which can be later processed and explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Data Munging tool\n",
    "import os # Operating System functionality\n",
    "import glob # Finds all pathnames e.g(.json)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Combines both Count Vectorizer and Tfidf Transformer\n",
    "import re # Regular Expression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users = pd.read_json(\"users.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36100-dsi\n",
      "36100-dsi-photos\n",
      "36103-statistics\n",
      "36106-dam\n",
      "File contains .json\n",
      "crisp-businessknow\n",
      "crisp-dataknow\n",
      "crisp-dataprep\n",
      "crisp-deploy\n",
      "crisp-eval\n",
      "crisp-modelling\n",
      "dam\n",
      "dsi\n",
      "dsi-assignment-2a\n",
      "general\n",
      "File contains .json\n",
      "jira-discussion\n",
      "python_instructions\n",
      "random\n",
      "story-telling\n",
      "File contains .json\n"
     ]
    }
   ],
   "source": [
    "path_to_json = 'C:/users/jchung/desktop/D-Matrix/'\n",
    "json_dir = os.listdir(path_to_json)\n",
    "#del json_dir[0] ### do this step only if u have a mac and need to get rid of .DS Store folder\n",
    "df3 = pd.DataFrame()\n",
    "for file in json_dir:\n",
    "    if \".json\" not in file: \n",
    "        print (file)\n",
    "        all_files = glob.glob(os.path.join(path_to_json + file + '/', \"*.json\"))\n",
    "        df = pd.concat((pd.read_json(f) for f in all_files)) \n",
    "        df['channel'] = file\n",
    "        df['ts'] = pd.to_datetime(df['ts'],unit='s')\n",
    "        df2 = pd.DataFrame(df, columns = ['user','ts','channel','text'])\n",
    "        df3 = df3.append(df2)\n",
    "    else:\n",
    "        print \"File contains .json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Script is from Martin and need to incorporate this into my script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "```# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu May 5 00:04:29 2016\n",
    "@author: Dmatrix\n",
    "\"\"\"\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import ijson\n",
    "crisp_mapper = {\n",
    "    'crisp-businessknow': 'Business Understanding',\n",
    "    'crisp-dataknow': 'Data Understanding',\n",
    "    'crisp-dataprep': 'Data Preparation',\n",
    "    'crisp-modelling': 'Modelling',\n",
    "    'crisp-eval': 'Evaluation',\n",
    "    'crisp-deploy': 'Deployment',\n",
    "    'story-telling': 'Deployment',\n",
    "    '36100-dsi': 'General',\n",
    "    '36100-dsi-photos': 'General',\n",
    "    '36103-statistics': 'General',\n",
    "    '36106-dam': 'General',\n",
    "    'dsi-assignment-2a': 'General',\n",
    "    'random': 'General',\n",
    "    'general': 'General'\n",
    "}\n",
    "user_mapper = {\n",
    "    'u0weaa087': 'Data Scientist 2',\n",
    "    'u0wdyeg6s': 'Business Analyst',\n",
    "    'u0x443ckz': 'Data Scientist 1',\n",
    "    'u0wfhmem8': 'Data Analytics Manager',\n",
    "    'u0x6cuq3d': 'Project Manager',\n",
    "    'u13aj8evd': 'Bender the Scrum Master',\n",
    "    'howdy': 'Bender the Scrum Master',\n",
    "    'duhita': 'Business Analyst',\n",
    "    'jared': 'Data Scientist 2',\n",
    "    'martin': 'Project Manager',\n",
    "    'hussam': 'Data Analytics Manager',\n",
    "    'chris': 'Data Scientist 1',\n",
    "    'chris.garces': 'Data Scientist 1',\n",
    "    'martinlehmann': 'Project Manager',\n",
    "    'ravi': 'Consultant'\n",
    "}\n",
    "def extract(directory='.\\data'):\n",
    "    \"\"\"Extract timestamp, user, text features from slack extracts\n",
    "    Args:\n",
    "        directory: The root directory folder where the slack extracts are located.\n",
    "    Returns:\n",
    "        a list of lists where each list contains the timestamp, user and text data.\n",
    "        Ignores Bot Messages that are not Howdy\n",
    "        Ignores File Comments\n",
    "        Ignores Channel creation messages\n",
    "        Ignores Channel purpose messages\n",
    "        Ignores file shares\n",
    "    \"\"\"\n",
    "    slack_records = []\n",
    "    row_filter = ['file_comment', 'bot_message', 'channel_purpose', 'channel_join', 'file_share']\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            crisp_activity = subdir.split(\"\\\\\")[2]  # HACK: get channel data\n",
    "            with open(os.path.join(subdir, file)) as json_file:\n",
    "                records = ijson.items(json_file, '')\n",
    "                for record in records:\n",
    "                    for message in record:\n",
    "                        if message.get('subtype') in row_filter and message.get('username') != 'howdy':\n",
    "                            continue\n",
    "                        user = message.get('user')\n",
    "                        # check if user is howdy\n",
    "                        if message.get('subtype') == 'bot_message' and user is None:\n",
    "                            user = message.get('username')\n",
    "                        current_record = [message.get('ts'),\n",
    "                                          user,\n",
    "                                          message.get('text'),\n",
    "                                          crisp_mapper.get(crisp_activity)]\n",
    "                        slack_records.append(current_record)\n",
    "    return slack_records\n",
    "def first_pass_cleaning(slack_records):\n",
    "    \"\"\" Transforms the slack record input addressing:\n",
    "        1) Transform slack timestamp to date\n",
    "        2) Transform email for positive sentiment of sharing\n",
    "        3) Transform links for positive sentiment of sharing\n",
    "        4) Remove unnecessary links from Howdy Bot\n",
    "        5) Mask user information to project role\n",
    "        6) Lower case words\n",
    "        7) Sorts records by time\n",
    "        8) Convert to ascii\n",
    "        spelling, abbreviations\n",
    "    Args:\n",
    "        slack_records: Raw input from slack extract.\n",
    "    Returns:\n",
    "        A list of cleaned data representing each slack message.\n",
    "    \"\"\"\n",
    "    for slack_record in slack_records:\n",
    "        # transform timestamp to date\n",
    "        slack_record[0] = timestamp_to_datetime(slack_record[0])\n",
    "        # transform email\n",
    "        slack_record[2] = mail_to_text(slack_record[2])\n",
    "        # Slack links to be changed to \"Sharing a link!\"\n",
    "        # exclamation for positive sentiment, sharing should be viewed positively\n",
    "        slack_record[2] = links_to_text(slack_record[2])\n",
    "        # Remove Bender's comments that are not english\n",
    "        slack_record[2] = bender_to_english(slack_record[2])\n",
    "        # transform user\n",
    "        slack_record[1] = user_mapper.get(str.lower(slack_record[1]))\n",
    "        # decode data\n",
    "        slack_record[2] = slack_record[2].encode('utf-8').decode('ascii', 'ignore')\n",
    "    # sort records by time\n",
    "    slack_records.sort(key=lambda x: datetime.datetime.strptime(x[0], '%Y-%m-%d %H:%M:%S'))\n",
    "    # convert to bag of words / or sentences\n",
    "    bag_of_words_list = []\n",
    "    #spelling\n",
    "    #abbreviation\n",
    "    for slack_record in slack_records:\n",
    "        # lower case message\n",
    "        slack_record[2] = str.lower(slack_record[2])\n",
    "        # mask user in message\n",
    "        for key, value in user_mapper.items():\n",
    "            slack_record[2] = slack_record[2].replace(key, value)\n",
    "        # Slack calling out a user: Replace \"<@user>:\" with \"user,\" or \"<@user>\" with \"user\"\n",
    "        for key, value in user_mapper.items():\n",
    "            slack_record[2] = slack_record[2].replace(str.format(\"<@{0}>:\", value), str.format(\"{0},\", value))\n",
    "            slack_record[2] = slack_record[2].replace(str.format(\"<@{0}>\", value), str.format(\"{0}\", value))\n",
    "    #TODO\n",
    "    return slack_records\n",
    "def replace_code_with_semantic_intent(message):\n",
    "    # if messasge contains : { [ ] }\n",
    "    nltk.tokenize(message)\n",
    "    return \"\"\n",
    "def bender_to_english(message):\n",
    "    return re.sub(r'<http://(d-matrix.)?howdy.ai/([a-zA-ez]+/)+([a-zA-Z]|[0-9])+[|]',\n",
    "                  '',\n",
    "                  message,\n",
    "                  re.I)\n",
    "def links_to_text(message):\n",
    "    return re.sub(r'((ht)|(f))tp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                  'Sharing a link!',\n",
    "                  message,\n",
    "                  re.I)\n",
    "def mail_to_text(message):\n",
    "    return re.sub(r'(mailto:)?[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+',\n",
    "                  'Here is my email!',\n",
    "                  message,\n",
    "                  re.I)\n",
    "def timestamp_to_datetime(timestamp):\n",
    "    return datetime.datetime.fromtimestamp(float(timestamp)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "def write_to_csv(records, output_file):\n",
    "    \"\"\"Writes list of slack records to CSV\n",
    "        Args:\n",
    "            records: The list containing all the messages\n",
    "            output_file: the filename csv output\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',', quotechar='\"')\n",
    "        writer.writerows(records)\n",
    "        #for record in records:\n",
    "            #writer.writerow([x.encode('utf-8') for x in record])\n",
    "         #   writer.writerow(record)\n",
    "def write_output_to_csv(bag_of_words_list, output_file):\n",
    "    \"\"\"Writes list of bag of words to CSV\n",
    "    Args:\n",
    "        bag_of_words_list: The list containing all the bag of words\n",
    "        output_file: the filename csv output\n",
    "    \"\"\"\n",
    "    with open(output_file, 'wb') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',', quotechar='\"')\n",
    "        for bag_of_words in bag_of_words_list:\n",
    "            writer.writerow(bag_of_words)  # TODO\n",
    "def main(source_directory, output_file):\n",
    "    slack_records = extract(source_directory)\n",
    "    #bag_of_words_list = transform(slack_records)\n",
    "    slack_records = first_pass_cleaning(slack_records)\n",
    "    write_to_csv(slack_records, output_file)\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1], sys.argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(s):\n",
    "    \n",
    "    s = re.sub(r'<http://(d-matrix.)?howdy.ai/([a-zA-ez]+/)+([a-zA-Z]|[0-9])+[|]','',s,re.I)\n",
    "    s = \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokeniser(s):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = pd.merge(df3,users,left_on='user',right_on='id',how='inner')\n",
    "data = data[['user','ts','channel','text','name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2940 entries, 0 to 2939\n",
      "Data columns (total 5 columns):\n",
      "user       2940 non-null object\n",
      "ts         2940 non-null datetime64[ns]\n",
      "channel    2940 non-null object\n",
      "text       2940 non-null object\n",
      "name       2940 non-null object\n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 137.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>ts</th>\n",
       "      <th>channel</th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U0WEAA087</td>\n",
       "      <td>2016-04-03 23:32:26.000002</td>\n",
       "      <td>36100-dsi</td>\n",
       "      <td>&lt;@U0WEAA087|jared&gt; has joined the channel</td>\n",
       "      <td>jared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U0WEAA087</td>\n",
       "      <td>2016-04-03 23:32:27.000003</td>\n",
       "      <td>36100-dsi</td>\n",
       "      <td>&lt;@U0WEAA087|jared&gt; set the channel purpose: Ch...</td>\n",
       "      <td>jared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U0WEAA087</td>\n",
       "      <td>2016-04-04 10:24:28.000009</td>\n",
       "      <td>36100-dsi</td>\n",
       "      <td>Yeah trying to get it done. I'm hoping to smas...</td>\n",
       "      <td>jared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U0WEAA087</td>\n",
       "      <td>2016-04-11 06:07:57.000119</td>\n",
       "      <td>36100-dsi</td>\n",
       "      <td>Hey Guys, sorry haven't been that active. I'm ...</td>\n",
       "      <td>jared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U0WEAA087</td>\n",
       "      <td>2016-04-11 06:11:39.000120</td>\n",
       "      <td>36100-dsi</td>\n",
       "      <td>&lt;http://nicomiceli.com/slackalytics/&gt; I might ...</td>\n",
       "      <td>jared</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user                         ts    channel  \\\n",
       "0  U0WEAA087 2016-04-03 23:32:26.000002  36100-dsi   \n",
       "1  U0WEAA087 2016-04-03 23:32:27.000003  36100-dsi   \n",
       "2  U0WEAA087 2016-04-04 10:24:28.000009  36100-dsi   \n",
       "3  U0WEAA087 2016-04-11 06:07:57.000119  36100-dsi   \n",
       "4  U0WEAA087 2016-04-11 06:11:39.000120  36100-dsi   \n",
       "\n",
       "                                                text   name  \n",
       "0          <@U0WEAA087|jared> has joined the channel  jared  \n",
       "1  <@U0WEAA087|jared> set the channel purpose: Ch...  jared  \n",
       "2  Yeah trying to get it done. I'm hoping to smas...  jared  \n",
       "3  Hey Guys, sorry haven't been that active. I'm ...  jared  \n",
       "4  <http://nicomiceli.com/slackalytics/> I might ...  jared  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crisp-dataprep         1159\n",
       "36100-dsi               542\n",
       "general                 301\n",
       "crisp-businessknow      249\n",
       "36106-dam               147\n",
       "python_instructions     138\n",
       "dsi-assignment-2a       135\n",
       "crisp-dataknow          116\n",
       "story-telling            95\n",
       "jira-discussion          91\n",
       "crisp-modelling          66\n",
       "random                   61\n",
       "36100-dsi-photos         44\n",
       "36103-statistics          6\n",
       "crisp-deploy              5\n",
       "crisp-eval                5\n",
       "dam                       2\n",
       "dsi                       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.channel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantified self - Jared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jared = data[data.name == 'jared']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
